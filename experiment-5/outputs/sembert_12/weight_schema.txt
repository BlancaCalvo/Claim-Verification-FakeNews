module.bert.embeddings.word_embeddings.weight torch.Size([30522, 768])
module.bert.embeddings.position_embeddings.weight torch.Size([512, 768])
module.bert.embeddings.token_type_embeddings.weight torch.Size([2, 768])
module.bert.embeddings.LayerNorm.weight torch.Size([768]) # not sure what layer norm is
module.bert.embeddings.LayerNorm.bias torch.Size([768])
module.bert.encoder.layer.0.attention.self.query.weight torch.Size([768, 768])
module.bert.encoder.layer.0.attention.self.query.bias torch.Size([768])
module.bert.encoder.layer.0.attention.self.key.weight torch.Size([768, 768])
module.bert.encoder.layer.0.attention.self.key.bias torch.Size([768])
module.bert.encoder.layer.0.attention.self.value.weight torch.Size([768, 768])
module.bert.encoder.layer.0.attention.self.value.bias torch.Size([768])
module.bert.encoder.layer.0.attention.output.dense.weight torch.Size([768, 768])
module.bert.encoder.layer.0.attention.output.dense.bias torch.Size([768])
module.bert.encoder.layer.0.attention.output.LayerNorm.weight torch.Size([768])
module.bert.encoder.layer.0.attention.output.LayerNorm.bias torch.Size([768])
module.bert.encoder.layer.0.intermediate.dense.weight torch.Size([3072, 768])
module.bert.encoder.layer.0.intermediate.dense.bias torch.Size([3072])
module.bert.encoder.layer.0.output.dense.weight torch.Size([768, 3072])
module.bert.encoder.layer.0.output.dense.bias torch.Size([768])
module.bert.encoder.layer.0.output.LayerNorm.weight torch.Size([768])
module.bert.encoder.layer.0.output.LayerNorm.bias torch.Size([768])
module.bert.encoder.layer.1.attention.self.query.weight torch.Size([768, 768])
module.bert.encoder.layer.1.attention.self.query.bias torch.Size([768])
module.bert.encoder.layer.1.attention.self.key.weight torch.Size([768, 768])
module.bert.encoder.layer.1.attention.self.key.bias torch.Size([768])
module.bert.encoder.layer.1.attention.self.value.weight torch.Size([768, 768])
module.bert.encoder.layer.1.attention.self.value.bias torch.Size([768])
module.bert.encoder.layer.1.attention.output.dense.weight torch.Size([768, 768])
module.bert.encoder.layer.1.attention.output.dense.bias torch.Size([768])
module.bert.encoder.layer.1.attention.output.LayerNorm.weight torch.Size([768])
module.bert.encoder.layer.1.attention.output.LayerNorm.bias torch.Size([768])
module.bert.encoder.layer.1.intermediate.dense.weight torch.Size([3072, 768])
module.bert.encoder.layer.1.intermediate.dense.bias torch.Size([3072])
module.bert.encoder.layer.1.output.dense.weight torch.Size([768, 3072])
module.bert.encoder.layer.1.output.dense.bias torch.Size([768])
module.bert.encoder.layer.1.output.LayerNorm.weight torch.Size([768])
module.bert.encoder.layer.1.output.LayerNorm.bias torch.Size([768])
module.bert.encoder.layer.2.attention.self.query.weight torch.Size([768, 768])
module.bert.encoder.layer.2.attention.self.query.bias torch.Size([768])
module.bert.encoder.layer.2.attention.self.key.weight torch.Size([768, 768])
module.bert.encoder.layer.2.attention.self.key.bias torch.Size([768])
module.bert.encoder.layer.2.attention.self.value.weight torch.Size([768, 768])
module.bert.encoder.layer.2.attention.self.value.bias torch.Size([768])
module.bert.encoder.layer.2.attention.output.dense.weight torch.Size([768, 768])
module.bert.encoder.layer.2.attention.output.dense.bias torch.Size([768])
module.bert.encoder.layer.2.attention.output.LayerNorm.weight torch.Size([768])
module.bert.encoder.layer.2.attention.output.LayerNorm.bias torch.Size([768])
module.bert.encoder.layer.2.intermediate.dense.weight torch.Size([3072, 768])
module.bert.encoder.layer.2.intermediate.dense.bias torch.Size([3072])
module.bert.encoder.layer.2.output.dense.weight torch.Size([768, 3072])
module.bert.encoder.layer.2.output.dense.bias torch.Size([768])
module.bert.encoder.layer.2.output.LayerNorm.weight torch.Size([768])
module.bert.encoder.layer.2.output.LayerNorm.bias torch.Size([768])
module.bert.encoder.layer.3.attention.self.query.weight torch.Size([768, 768])
module.bert.encoder.layer.3.attention.self.query.bias torch.Size([768])
module.bert.encoder.layer.3.attention.self.key.weight torch.Size([768, 768])
module.bert.encoder.layer.3.attention.self.key.bias torch.Size([768])
module.bert.encoder.layer.3.attention.self.value.weight torch.Size([768, 768])
module.bert.encoder.layer.3.attention.self.value.bias torch.Size([768])
module.bert.encoder.layer.3.attention.output.dense.weight torch.Size([768, 768])
module.bert.encoder.layer.3.attention.output.dense.bias torch.Size([768])
module.bert.encoder.layer.3.attention.output.LayerNorm.weight torch.Size([768])
module.bert.encoder.layer.3.attention.output.LayerNorm.bias torch.Size([768])
module.bert.encoder.layer.3.intermediate.dense.weight torch.Size([3072, 768])
module.bert.encoder.layer.3.intermediate.dense.bias torch.Size([3072])
module.bert.encoder.layer.3.output.dense.weight torch.Size([768, 3072])
module.bert.encoder.layer.3.output.dense.bias torch.Size([768])
module.bert.encoder.layer.3.output.LayerNorm.weight torch.Size([768])
module.bert.encoder.layer.3.output.LayerNorm.bias torch.Size([768])
module.bert.encoder.layer.4.attention.self.query.weight torch.Size([768, 768])
module.bert.encoder.layer.4.attention.self.query.bias torch.Size([768])
module.bert.encoder.layer.4.attention.self.key.weight torch.Size([768, 768])
module.bert.encoder.layer.4.attention.self.key.bias torch.Size([768])
module.bert.encoder.layer.4.attention.self.value.weight torch.Size([768, 768])
module.bert.encoder.layer.4.attention.self.value.bias torch.Size([768])
module.bert.encoder.layer.4.attention.output.dense.weight torch.Size([768, 768])
module.bert.encoder.layer.4.attention.output.dense.bias torch.Size([768])
module.bert.encoder.layer.4.attention.output.LayerNorm.weight torch.Size([768])
module.bert.encoder.layer.4.attention.output.LayerNorm.bias torch.Size([768])
module.bert.encoder.layer.4.intermediate.dense.weight torch.Size([3072, 768])
module.bert.encoder.layer.4.intermediate.dense.bias torch.Size([3072])
module.bert.encoder.layer.4.output.dense.weight torch.Size([768, 3072])
module.bert.encoder.layer.4.output.dense.bias torch.Size([768])
module.bert.encoder.layer.4.output.LayerNorm.weight torch.Size([768])
module.bert.encoder.layer.4.output.LayerNorm.bias torch.Size([768])
module.bert.encoder.layer.5.attention.self.query.weight torch.Size([768, 768])
module.bert.encoder.layer.5.attention.self.query.bias torch.Size([768])
module.bert.encoder.layer.5.attention.self.key.weight torch.Size([768, 768])
module.bert.encoder.layer.5.attention.self.key.bias torch.Size([768])
module.bert.encoder.layer.5.attention.self.value.weight torch.Size([768, 768])
module.bert.encoder.layer.5.attention.self.value.bias torch.Size([768])
module.bert.encoder.layer.5.attention.output.dense.weight torch.Size([768, 768])
module.bert.encoder.layer.5.attention.output.dense.bias torch.Size([768])
module.bert.encoder.layer.5.attention.output.LayerNorm.weight torch.Size([768])
module.bert.encoder.layer.5.attention.output.LayerNorm.bias torch.Size([768])
module.bert.encoder.layer.5.intermediate.dense.weight torch.Size([3072, 768])
module.bert.encoder.layer.5.intermediate.dense.bias torch.Size([3072])
module.bert.encoder.layer.5.output.dense.weight torch.Size([768, 3072])
module.bert.encoder.layer.5.output.dense.bias torch.Size([768])
module.bert.encoder.layer.5.output.LayerNorm.weight torch.Size([768])
module.bert.encoder.layer.5.output.LayerNorm.bias torch.Size([768])
module.bert.encoder.layer.6.attention.self.query.weight torch.Size([768, 768])
module.bert.encoder.layer.6.attention.self.query.bias torch.Size([768])
module.bert.encoder.layer.6.attention.self.key.weight torch.Size([768, 768])
module.bert.encoder.layer.6.attention.self.key.bias torch.Size([768])
module.bert.encoder.layer.6.attention.self.value.weight torch.Size([768, 768])
module.bert.encoder.layer.6.attention.self.value.bias torch.Size([768])
module.bert.encoder.layer.6.attention.output.dense.weight torch.Size([768, 768])
module.bert.encoder.layer.6.attention.output.dense.bias torch.Size([768])
module.bert.encoder.layer.6.attention.output.LayerNorm.weight torch.Size([768])
module.bert.encoder.layer.6.attention.output.LayerNorm.bias torch.Size([768])
module.bert.encoder.layer.6.intermediate.dense.weight torch.Size([3072, 768])
module.bert.encoder.layer.6.intermediate.dense.bias torch.Size([3072])
module.bert.encoder.layer.6.output.dense.weight torch.Size([768, 3072])
module.bert.encoder.layer.6.output.dense.bias torch.Size([768])
module.bert.encoder.layer.6.output.LayerNorm.weight torch.Size([768])
module.bert.encoder.layer.6.output.LayerNorm.bias torch.Size([768])
module.bert.encoder.layer.7.attention.self.query.weight torch.Size([768, 768])
module.bert.encoder.layer.7.attention.self.query.bias torch.Size([768])
module.bert.encoder.layer.7.attention.self.key.weight torch.Size([768, 768])
module.bert.encoder.layer.7.attention.self.key.bias torch.Size([768])
module.bert.encoder.layer.7.attention.self.value.weight torch.Size([768, 768])
module.bert.encoder.layer.7.attention.self.value.bias torch.Size([768])
module.bert.encoder.layer.7.attention.output.dense.weight torch.Size([768, 768])
module.bert.encoder.layer.7.attention.output.dense.bias torch.Size([768])
module.bert.encoder.layer.7.attention.output.LayerNorm.weight torch.Size([768])
module.bert.encoder.layer.7.attention.output.LayerNorm.bias torch.Size([768])
module.bert.encoder.layer.7.intermediate.dense.weight torch.Size([3072, 768])
module.bert.encoder.layer.7.intermediate.dense.bias torch.Size([3072])
module.bert.encoder.layer.7.output.dense.weight torch.Size([768, 3072])
module.bert.encoder.layer.7.output.dense.bias torch.Size([768])
module.bert.encoder.layer.7.output.LayerNorm.weight torch.Size([768])
module.bert.encoder.layer.7.output.LayerNorm.bias torch.Size([768])
module.bert.encoder.layer.8.attention.self.query.weight torch.Size([768, 768])
module.bert.encoder.layer.8.attention.self.query.bias torch.Size([768])
module.bert.encoder.layer.8.attention.self.key.weight torch.Size([768, 768])
module.bert.encoder.layer.8.attention.self.key.bias torch.Size([768])
module.bert.encoder.layer.8.attention.self.value.weight torch.Size([768, 768])
module.bert.encoder.layer.8.attention.self.value.bias torch.Size([768])
module.bert.encoder.layer.8.attention.output.dense.weight torch.Size([768, 768])
module.bert.encoder.layer.8.attention.output.dense.bias torch.Size([768])
module.bert.encoder.layer.8.attention.output.LayerNorm.weight torch.Size([768])
module.bert.encoder.layer.8.attention.output.LayerNorm.bias torch.Size([768])
module.bert.encoder.layer.8.intermediate.dense.weight torch.Size([3072, 768])
module.bert.encoder.layer.8.intermediate.dense.bias torch.Size([3072])
module.bert.encoder.layer.8.output.dense.weight torch.Size([768, 3072])
module.bert.encoder.layer.8.output.dense.bias torch.Size([768])
module.bert.encoder.layer.8.output.LayerNorm.weight torch.Size([768])
module.bert.encoder.layer.8.output.LayerNorm.bias torch.Size([768])
module.bert.encoder.layer.9.attention.self.query.weight torch.Size([768, 768])
module.bert.encoder.layer.9.attention.self.query.bias torch.Size([768])
module.bert.encoder.layer.9.attention.self.key.weight torch.Size([768, 768])
module.bert.encoder.layer.9.attention.self.key.bias torch.Size([768])
module.bert.encoder.layer.9.attention.self.value.weight torch.Size([768, 768])
module.bert.encoder.layer.9.attention.self.value.bias torch.Size([768])
module.bert.encoder.layer.9.attention.output.dense.weight torch.Size([768, 768])
module.bert.encoder.layer.9.attention.output.dense.bias torch.Size([768])
module.bert.encoder.layer.9.attention.output.LayerNorm.weight torch.Size([768])
module.bert.encoder.layer.9.attention.output.LayerNorm.bias torch.Size([768])
module.bert.encoder.layer.9.intermediate.dense.weight torch.Size([3072, 768])
module.bert.encoder.layer.9.intermediate.dense.bias torch.Size([3072])
module.bert.encoder.layer.9.output.dense.weight torch.Size([768, 3072])
module.bert.encoder.layer.9.output.dense.bias torch.Size([768])
module.bert.encoder.layer.9.output.LayerNorm.weight torch.Size([768])
module.bert.encoder.layer.9.output.LayerNorm.bias torch.Size([768])
module.bert.encoder.layer.10.attention.self.query.weight torch.Size([768, 768])
module.bert.encoder.layer.10.attention.self.query.bias torch.Size([768])
module.bert.encoder.layer.10.attention.self.key.weight torch.Size([768, 768])
module.bert.encoder.layer.10.attention.self.key.bias torch.Size([768])
module.bert.encoder.layer.10.attention.self.value.weight torch.Size([768, 768])
module.bert.encoder.layer.10.attention.self.value.bias torch.Size([768])
module.bert.encoder.layer.10.attention.output.dense.weight torch.Size([768, 768])
module.bert.encoder.layer.10.attention.output.dense.bias torch.Size([768])
module.bert.encoder.layer.10.attention.output.LayerNorm.weight torch.Size([768])
module.bert.encoder.layer.10.attention.output.LayerNorm.bias torch.Size([768])
module.bert.encoder.layer.10.intermediate.dense.weight torch.Size([3072, 768])
module.bert.encoder.layer.10.intermediate.dense.bias torch.Size([3072])
module.bert.encoder.layer.10.output.dense.weight torch.Size([768, 3072])
module.bert.encoder.layer.10.output.dense.bias torch.Size([768])
module.bert.encoder.layer.10.output.LayerNorm.weight torch.Size([768])
module.bert.encoder.layer.10.output.LayerNorm.bias torch.Size([768])
module.bert.encoder.layer.11.attention.self.query.weight torch.Size([768, 768])
module.bert.encoder.layer.11.attention.self.query.bias torch.Size([768])
module.bert.encoder.layer.11.attention.self.key.weight torch.Size([768, 768])
module.bert.encoder.layer.11.attention.self.key.bias torch.Size([768])
module.bert.encoder.layer.11.attention.self.value.weight torch.Size([768, 768])
module.bert.encoder.layer.11.attention.self.value.bias torch.Size([768])
module.bert.encoder.layer.11.attention.output.dense.weight torch.Size([768, 768])
module.bert.encoder.layer.11.attention.output.dense.bias torch.Size([768])
module.bert.encoder.layer.11.attention.output.LayerNorm.weight torch.Size([768])
module.bert.encoder.layer.11.attention.output.LayerNorm.bias torch.Size([768])
module.bert.encoder.layer.11.intermediate.dense.weight torch.Size([3072, 768])
module.bert.encoder.layer.11.intermediate.dense.bias torch.Size([3072])
module.bert.encoder.layer.11.output.dense.weight torch.Size([768, 3072])
module.bert.encoder.layer.11.output.dense.bias torch.Size([768])
module.bert.encoder.layer.11.output.LayerNorm.weight torch.Size([768])
module.bert.encoder.layer.11.output.LayerNorm.bias torch.Size([768])
module.bert.pooler.dense.weight torch.Size([768, 768])
module.bert.pooler.dense.bias torch.Size([768])
module.cnn.char_cnn.weight torch.Size([768, 768, 3])
module.cnn.char_cnn.bias torch.Size([768])
module.tag_model.embed.tag_embeddings.weight torch.Size([22, 10]) # with this I can print the embeddings of the tags
module.tag_model.embed.LayerNorm.weight torch.Size([10]) # I assume this is layer normalisation
module.tag_model.embed.LayerNorm.bias torch.Size([10])
module.tag_model.fc.weight torch.Size([10, 10])
module.tag_model.fc.bias torch.Size([10])
module.dense.weight torch.Size([10, 120]) # with this I could check which embeddings of which propositions are given more weight
module.dense.bias torch.Size([10])
module.pool.weight torch.Size([778, 778])
module.pool.bias torch.Size([778])
module.classifier.weight torch.Size([3, 778])
module.classifier.bias torch.Size([3])
